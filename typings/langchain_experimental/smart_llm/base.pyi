"""
This type stub file was generated by pyright.
"""

from typing import Any, Dict, List, Optional, Tuple, Type
from langchain.base_language import BaseLanguageModel
from langchain.callbacks.manager import CallbackManagerForChainRun
from langchain.chains.base import Chain
from langchain.prompts.base import BasePromptTemplate
from langchain.prompts.chat import BaseMessagePromptTemplate, ChatPromptTemplate
from langchain.schema import PromptValue
from langchain_experimental.pydantic_v1 import root_validator

"""Chain for applying self-critique using the SmartGPT workflow."""
class SmartLLMChain(Chain):
    """
    Generalized implementation of SmartGPT (origin: https://youtu.be/wVzuvf9D9BU)

    A SmartLLMChain is an LLMChain that instead of simply passing the prompt to the LLM
    performs these 3 steps:
    1. Ideate: Pass the user prompt to an ideation LLM n_ideas times,
       each result is an "idea"
    2. Critique: Pass the ideas to a critique LLM which looks for flaws in the ideas
       & picks the best one
    3. Resolve: Pass the critique to a resolver LLM which improves upon the best idea
       & outputs only the (improved version of) the best output

    In total, SmartLLMChain pass will use n_ideas+2 LLM calls

    Note that SmartLLMChain will only improve results (compared to a basic LLMChain),
    when the underlying models have the capability for reflection, which smaller models
    often don't.

    Finally, a SmartLLMChain assumes that each underlying LLM outputs exactly 1 result.
    """
    class SmartLLMChainHistory:
        question: str = ...
        ideas: List[str] = ...
        critique: str = ...
        @property
        def n_ideas(self) -> int:
            ...
        
        def ideation_prompt_inputs(self) -> Dict[str, Any]:
            ...
        
        def critique_prompt_inputs(self) -> Dict[str, Any]:
            ...
        
        def resolve_prompt_inputs(self) -> Dict[str, Any]:
            ...
        
    
    
    prompt: BasePromptTemplate
    ideation_llm: Optional[BaseLanguageModel] = ...
    critique_llm: Optional[BaseLanguageModel] = ...
    resolver_llm: Optional[BaseLanguageModel] = ...
    llm: Optional[BaseLanguageModel] = ...
    n_ideas: int = ...
    return_intermediate_steps: bool = ...
    history: SmartLLMChainHistory = ...
    class Config:
        extra = ...
    
    
    @root_validator
    @classmethod
    def validate_inputs(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        """Ensure we have an LLM for each step."""
        ...
    
    @property
    def input_keys(self) -> List[str]:
        """Defines the input keys."""
        ...
    
    @property
    def output_keys(self) -> List[str]:
        """Defines the output keys."""
        ...
    
    def prep_prompts(self, inputs: Dict[str, Any], run_manager: Optional[CallbackManagerForChainRun] = ...) -> Tuple[PromptValue, Optional[List[str]]]:
        """Prepare prompts from inputs."""
        ...
    
    def get_prompt_strings(self, stage: str) -> List[Tuple[Type[BaseMessagePromptTemplate], str]]:
        ...
    
    def ideation_prompt(self) -> ChatPromptTemplate:
        ...
    
    def critique_prompt(self) -> ChatPromptTemplate:
        ...
    
    def resolve_prompt(self) -> ChatPromptTemplate:
        ...
    


